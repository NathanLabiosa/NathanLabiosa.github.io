---
title: 'August Paper Review'
date: 2025-09-03
permalink: /posts/2025/09/blog-post-1/
tags:
  - LLMs
  - Computer Vision
  - Context Windows
---

Hello! I'm trying something new here on this blog. I'm challenging myself to read a paper a day (not something that seems very hard) to try to understand and build upon them. To keep myself accountable, I come up with a few things about the main ideas and key takeaways. At the end of each month, I want to put together a post about the most interesting few papers I read. We'll see how long my motivation for this lasts ^-^


LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning
------
Citation: H. Jin et al., "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning," in Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria, Jul. 2024.

**Brief Overview:**

Large Language Models struggle with inputs longer than their training sequence length, limiting their ability to handle long-context tasks. The authors propose SelfExtend, a method to extend an LLM’s context window without fine-tuning. They introduce bi-level attention during inference in two ways. The first is grouped attention: Models relationships between tokens that are far apart. Neighbor attention: Focuses on adjacent tokens within a local range. Both attentions reuse the model’s original self-attention mechanism, only requiring minor code changes rather than retraining.

**What I found interesting:**

I thought it was interesting how simple this method was, and how it played with positional embeddings. They made a distinction between close and far tokens, and for the close ones they can use standard attention to preseve the local coherence. But for the far ones, they use a simple floor division operation to compress the distant toeksn into groups. Then they just merge these two together for the final attention. The transformer architecture is complicated, and I like to see how people are exploring individual parts of it to iteratively make it perform better.

Knowledge Fusion of Large Language Models
------
Citation: F. Wan et al., "KNOWLEDGE FUSION OF LARGE LANGUAGE MODELS," in Proc. ICLR, Jan. 2024.

**Brief Overview:**

This paper introduces FUSELLM, a method for "knowledge fusion" of large language models. The goal is to combine the capabilities of existing, pre-trained LLMs into a single, more powerful model. It works by using several "source" LLMs by taking advantage of their generative probabilistic distributions. This knowledge is then transferred to a "target" LLM through lightweight continual training on a compact, high-quality corpus. This continual training aims to minimize the divergence between the target LLM's output distributions and the fused distributions from the source models. The paper validates this approach using three models with different architectures: Llama-2, MPT, and OpenLLaMA.

**What I found interesting:**

I think that knowledge distillation is pretty cool. My first real exposure to research was in knowledge distillation. It's interesting how instead of merging the model weights and doing some sort of weighing there, they go to the distributions instead. I do wonder how these distributions all combine in space, but theoretically they should all be trying to describe the same thing. Then using a combined version of this, they show that a new model can pull the best of all worlds by modeling this distribution.


Tuning Language Models by Proxy
------
Citation: A. Liu et al., "Tuning Language Models by Proxy," in Proc. COLM, Aug. 2024.

**Brief Overview:**

The main contribution of this paper is proxy-tuning, a lightweight decoding-time algorithm that enables the "tuning" of large language models without accessing their internal weights or parameters. They use a smaller LLM as a "proxy". Then the difference between the logits of this small, tuned model and its untuned version is calculated. The difference aka logit offset, is then applied to the output predictions of the large, untuned base model at decoding time. This process effectively steers the larger model's output in the direction of the tuning, while allowing it to retain the benefits of its large-scale pretraining.


**What I found interesting:**

I like the idea of using smaller models to steer the outputs of larger ones, as usually it's the other way around. I do like this idea and the work, they basically ensure that if the expert model is better, the logits will be appropriately combined by doing (sm+ - sm-). They also showed that they can implement updated information past the training cut off to older models. That's cool, it'll save a lot of time and energy just to get more updated information on something huge like ChatGPT.


Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon
------
Citation: P. Zhang et al., "Long Context Compression with Activation Beacon," arXiv e-prints, vol. 2401.03462, Oct. 2024.

**Brief Overview:**

This paper is about Activation Beacon, a plug-in module for LLMs that tries to compress long contexts. Instead of using "soft prompts" to summarize information, Activation Beacon directly compresses the activations (the keys and values) at every layer. This in theory allows it to encapsulate more complex information from the long contexts. Long contexts are first divided into chunks. Within each chunk, "beacon tokens" are inserted with the input tokens. The LLM encodes one chunk at a time, and the contextual information is distilled into the activations of these beacon tokens. The activations of the original raw tokens are then discarded, while the beacon token activations are accumulated and reused for encoding subsequent chunks.

**What I found interesting:**

I thought it was interesting how they can distill the contextual information of the chunk into the activations (keys and values) of the beacon tokens. They can basically compress all the previous information into a set of tokens. This is like the previous paper I mentioned, LLM Maybe LongLM, where we're trying to better compress information in the encoding layer. That way we can give more valuable input to the model and hopefully get better output.


VMamba: Visual State Space Model
------
Citation: Y. Liu et al., "VMamba: Visual State Space Model," arXiv e-prints, vol. 2401.10166, Dec. 2024.

**Brief Overview:**

This paper is about VMamba, a vision backbone with linear time complexity, designed to be more computationally efficient for computer vision tasks. It adapts the Mamba State Space Model to handle two-dimensional visual data. The core component of VMamba is the 2D Selective Scan (SS2D) module. This module addresses the challenge of applying the 1D sequential scanning nature of Mamba to the non-sequential structure of 2D images. SS2D works by traversing image patches along four different scanning routes. Each sequence is processed by a separate S6 block, and the results are then merged to create a 2D feature map. This process allows each image patch to gather contextual information from all other pixels, establishing a global receptive field in the 2D space while reducing the computational complexity from quadratic to linear.

**What I found interesting:**

This is my first time reading about state space models applied to computer vision. It seems like a pretty good upgrade if we can maintain performance and speed up computation. I also worked on a project that has a similar idea applied to video stability, where we have the output of the previous state, then we have a learned parameter weigh the previous output and current input to provide some stability from frame to frame. In these state space models, it seems like the future state can be determined by the current state and any new inputs.


